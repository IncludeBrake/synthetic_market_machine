# SMVM Release Gate Runbook

## Overview

This runbook defines the comprehensive release gate procedures for the Synthetic Market Validation Module (SMVM). The release gate ensures that only ship-ready, thoroughly validated software with decisive, evidence-backed recommendations reaches production deployment.

## Release Gate Philosophy

### Core Principles
- **Zero-Trust Validation**: Every release undergoes complete validation regardless of prior testing
- **Evidence-Based Decisions**: All Go/Pivot/Kill recommendations must be backed by reproducible evidence
- **Version Consistency**: Releases must maintain python_version consistency across all components
- **Token Budget Compliance**: All operations must respect established token ceilings
- **Audit Trail Completeness**: Full provenance tracking from data ingestion to final recommendations

### Gate Philosophy
The release gate operates on a "fail-closed" model where any single failure blocks the release. This ensures that only systems meeting the highest standards of quality, security, and reliability reach production.

## Release Gate Criteria

### Gate 1: Contract Compliance Validation

#### Objective
Verify that all public interfaces and data contracts are properly implemented and validated.

#### Automated Checks
```bash
#!/bin/bash
# contract_validation_gate.sh - Contract compliance validation

echo "=== CONTRACT COMPLIANCE VALIDATION ==="

# Check 1: Schema Validation
echo "1. Schema Validation..."
python contracts/validate_schemas.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: Schema validation failed"
    exit 1
fi

# Check 2: Unknown Key Rejection
echo "2. Unknown Key Rejection..."
python contracts/test_unknown_keys.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: Unknown key rejection failed"
    exit 1
fi

# Check 3: Fixture Round-trip Validation
echo "3. Fixture Round-trip Validation..."
python contracts/test_fixtures.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: Fixture validation failed"
    exit 1
fi

echo "‚úÖ CONTRACT COMPLIANCE: PASSED"
```

#### Manual Verification
- [ ] All JSON schemas validate without errors
- [ ] Unknown keys are rejected in all contracts
- [ ] All fixture data conforms to schemas
- [ ] Schema versioning is properly documented

#### Failure Impact
**BLOCKS RELEASE**: Contract violations indicate fundamental interface incompatibilities that could cause system failures in production.

### Gate 2: Determinism and Reproducibility Validation

#### Objective
Ensure all SMVM outputs are deterministic and reproducible under identical conditions.

#### Automated Checks
```python
#!/usr/bin/env python3
# determinism_gate.py - Determinism validation

import sys
import json
from pathlib import Path

def validate_determinism():
    """Validate determinism across multiple runs"""

    print("=== DETERMINISM VALIDATION ===")

    # Run 1
    result1 = run_smvm_simulation(seed=42)
    hash1 = compute_output_hash(result1)

    # Run 2 (same seed)
    result2 = run_smvm_simulation(seed=42)
    hash2 = compute_output_hash(result2)

    # Run 3 (different seed)
    result3 = run_smvm_simulation(seed=123)
    hash3 = compute_output_hash(result3)

    # Validation
    if hash1 != hash2:
        print("‚ùå GATE BLOCKED: Non-deterministic results with same seed")
        print(f"Hash1: {hash1}")
        print(f"Hash2: {hash2}")
        return False

    if hash2 == hash3:
        print("‚ùå GATE BLOCKED: Deterministic results with different seeds")
        return False

    print("‚úÖ DETERMINISM: PASSED")
    return True

def validate_replay():
    """Validate replay functionality"""

    print("=== REPLAY VALIDATION ===")

    # Original run
    original_result = run_smvm_simulation(seed=42)
    original_meta = extract_metadata(original_result)

    # Replay with same python_version
    replay_result = replay_smvm_simulation(run_id=original_meta['run_id'])
    replay_meta = extract_metadata(replay_result)

    # Validate replay consistency
    if original_meta['python_version'] != replay_meta['python_version']:
        print("‚ùå GATE BLOCKED: Python version mismatch in replay")
        return False

    if original_result != replay_result:
        print("‚ùå GATE BLOCKED: Replay results differ from original")
        return False

    print("‚úÖ REPLAY: PASSED")
    return True

if __name__ == "__main__":
    if not validate_determinism():
        sys.exit(1)

    if not validate_replay():
        sys.exit(1)

    print("üéâ DETERMINISM GATE: PASSED")
```

#### Manual Verification
- [ ] Multiple runs with same seed produce identical results
- [ ] Different seeds produce different but valid results
- [ ] Replay functionality works with same python_version
- [ ] Output hashes are consistent across identical runs

#### Failure Impact
**BLOCKS RELEASE**: Non-deterministic behavior indicates underlying system instability that could cause unpredictable results in production.

### Gate 3: Token Budget Compliance Validation

#### Objective
Ensure all LLM operations respect established token ceilings and budget constraints.

#### Automated Checks
```python
#!/usr/bin/env python3
# token_budget_gate.py - Token budget compliance validation

import json
from pathlib import Path

class TokenBudgetValidator:
    """Validate token budget compliance across all operations"""

    def __init__(self):
        self.budgets = {
            'ingestion': 1000,
            'personas': 2000,
            'competitors': 1500,
            'simulation': 5000,
            'analysis': 3000,
            'total_system': 10000
        }

        self.token_usage = {}

    def validate_token_compliance(self) -> bool:
        """Validate token usage against budgets"""

        print("=== TOKEN BUDGET COMPLIANCE ===")

        # Load token usage from logs
        token_logs = self._load_token_logs()

        # Validate per-component budgets
        for component, budget in self.budgets.items():
            if component == 'total_system':
                continue

            usage = sum(log['tokens_used'] for log in token_logs
                       if log['component'] == component)

            if usage > budget:
                print(f"‚ùå GATE BLOCKED: {component} exceeded budget")
                print(f"  Budget: {budget}, Used: {usage}")
                return False

            print(f"‚úÖ {component}: {usage}/{budget} tokens")

        # Validate total system budget
        total_usage = sum(log['tokens_used'] for log in token_logs)
        total_budget = self.budgets['total_system']

        if total_usage > total_budget:
            print("‚ùå GATE BLOCKED: Total system exceeded budget")
            print(f"  Budget: {total_budget}, Used: {total_usage}")
            return False

        print(f"‚úÖ Total System: {total_usage}/{total_budget} tokens")
        print("‚úÖ TOKEN BUDGET: PASSED")
        return True

    def _load_token_logs(self):
        """Load token usage logs"""
        logs = []
        log_files = Path("logs").glob("*.jsonl")

        for log_file in log_files:
            with open(log_file, 'r') as f:
                for line in f:
                    if line.strip():
                        log_entry = json.loads(line)
                        if 'tokens_used' in log_entry:
                            logs.append(log_entry)

        return logs

if __name__ == "__main__":
    validator = TokenBudgetValidator()
    if not validator.validate_token_compliance():
        exit(1)
    print("üéâ TOKEN BUDGET GATE: PASSED")
```

#### Manual Verification
- [ ] All component token budgets are respected
- [ ] Total system token budget is not exceeded
- [ ] Token usage is properly logged and tracked
- [ ] Budget violations trigger appropriate alerts

#### Failure Impact
**BLOCKS RELEASE**: Token budget violations indicate potential cost overruns and performance issues in production.

### Gate 4: Decision Quality Validation

#### Objective
Ensure Go/Pivot/Kill recommendations are evidence-based and meet quality thresholds.

#### Automated Checks
```python
#!/usr/bin/env python3
# decision_quality_gate.py - Decision quality validation

import json
from pathlib import Path

class DecisionQualityValidator:
    """Validate quality of Go/Pivot/Kill recommendations"""

    def __init__(self):
        self.quality_thresholds = {
            'evidence_score': 0.8,      # Minimum evidence quality score
            'confidence_level': 0.7,    # Minimum confidence threshold
            'data_coverage': 0.9,       # Minimum data coverage
            'bias_check_passed': True,  # Bias validation must pass
            'reproducibility_score': 1.0  # Must be 100% reproducible
        }

    def validate_decision_quality(self) -> bool:
        """Validate decision quality meets thresholds"""

        print("=== DECISION QUALITY VALIDATION ===")

        # Load latest validation report
        report_path = Path("reports/validation_report.md")
        if not report_path.exists():
            print("‚ùå GATE BLOCKED: Validation report not found")
            return False

        # Parse report content
        with open(report_path, 'r') as f:
            report_content = f.read()

        # Extract decision and metrics
        decision_metrics = self._extract_decision_metrics(report_content)

        # Validate against thresholds
        for metric, threshold in self.quality_thresholds.items():
            actual_value = decision_metrics.get(metric, 0)

            if isinstance(threshold, bool):
                if actual_value != threshold:
                    print(f"‚ùå GATE BLOCKED: {metric} failed")
                    print(f"  Required: {threshold}, Actual: {actual_value}")
                    return False
            elif actual_value < threshold:
                print(f"‚ùå GATE BLOCKED: {metric} below threshold")
                print(".2f")
                return False

            if isinstance(threshold, bool):
                print(f"‚úÖ {metric}: {'PASS' if actual_value else 'FAIL'}")
            else:
                print(".2f")

        print("‚úÖ DECISION QUALITY: PASSED")
        return True

    def _extract_decision_metrics(self, report_content: str) -> dict:
        """Extract decision metrics from report"""

        metrics = {}

        # Look for key metrics in report (simplified parsing)
        lines = report_content.split('\n')

        for line in lines:
            line = line.strip()
            if 'Evidence Score:' in line:
                metrics['evidence_score'] = float(line.split(':')[1].strip())
            elif 'Confidence Level:' in line:
                metrics['confidence_level'] = float(line.split(':')[1].strip())
            elif 'Data Coverage:' in line:
                metrics['data_coverage'] = float(line.split(':')[1].strip())
            elif 'Bias Check:' in line:
                metrics['bias_check_passed'] = 'PASS' in line
            elif 'Reproducibility:' in line:
                metrics['reproducibility_score'] = float(line.split(':')[1].strip())

        return metrics

if __name__ == "__main__":
    validator = DecisionQualityValidator()
    if not validator.validate_decision_quality():
        exit(1)
    print("üéâ DECISION QUALITY GATE: PASSED")
```

#### Manual Verification
- [ ] Evidence score meets minimum threshold (‚â•0.8)
- [ ] Confidence level is adequate (‚â•0.7)
- [ ] Data coverage is comprehensive (‚â•0.9)
- [ ] Bias checks have passed
- [ ] Results are 100% reproducible

#### Failure Impact
**BLOCKS RELEASE**: Poor quality decisions could lead to incorrect business recommendations with significant financial impact.

### Gate 5: Security and Compliance Validation

#### Objective
Ensure all security requirements and compliance standards are met.

#### Automated Checks
```bash
#!/bin/bash
# security_gate.sh - Security and compliance validation

echo "=== SECURITY & COMPLIANCE VALIDATION ==="

# Check 1: Secrets Management
echo "1. Secrets Management..."
python security/check_secrets.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: Secrets management validation failed"
    exit 1
fi

# Check 2: RBAC Compliance
echo "2. RBAC Compliance..."
python security/check_rbac.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: RBAC compliance failed"
    exit 1
fi

# Check 3: Data Zone Integrity
echo "3. Data Zone Integrity..."
python security/check_zones.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: Data zone integrity failed"
    exit 1
fi

# Check 4: GDPR Compliance
echo "4. GDPR Compliance..."
python compliance/check_gdpr.py
if [ $? -ne 0 ]; then
    echo "‚ùå GATE BLOCKED: GDPR compliance failed"
    exit 1
fi

echo "‚úÖ SECURITY & COMPLIANCE: PASSED"
```

#### Manual Verification
- [ ] No secrets are present in repository
- [ ] RBAC permissions are properly configured
- [ ] Data zone boundaries are maintained
- [ ] GDPR requirements are satisfied
- [ ] Security scans pass without critical issues

#### Failure Impact
**BLOCKS RELEASE**: Security vulnerabilities or compliance failures pose unacceptable risks to production systems.

### Gate 6: Python Version Consistency Validation

#### Objective
Ensure python_version consistency across all components and operations.

#### Automated Checks
```python
#!/usr/bin/env python3
# python_version_gate.py - Python version consistency validation

import sys
import json
import subprocess
from pathlib import Path

class PythonVersionValidator:
    """Validate Python version consistency"""

    def __init__(self):
        self.required_version = "3.12"  # Primary version
        self.fallback_version = "3.11.13"
        self.allowed_versions = [self.required_version, self.fallback_version]

    def validate_version_consistency(self) -> bool:
        """Validate Python version consistency across system"""

        print("=== PYTHON VERSION CONSISTENCY ===")

        # Check current environment
        current_version = self._get_current_python_version()
        if current_version not in self.allowed_versions:
            print(f"‚ùå GATE BLOCKED: Current Python {current_version} not allowed")
            return False

        print(f"‚úÖ Current Python version: {current_version}")

        # Check virtual environment consistency
        venv_version = self._get_venv_python_version()
        if venv_version != current_version:
            print(f"‚ùå GATE BLOCKED: Venv version {venv_version} != current {current_version}")
            return False

        print(f"‚úÖ Virtual environment version: {venv_version}")

        # Check logged versions in recent runs
        log_versions = self._get_logged_versions()
        for log_version in log_versions:
            if log_version not in self.allowed_versions:
                print(f"‚ùå GATE BLOCKED: Log contains invalid version {log_version}")
                return False

        print(f"‚úÖ Log versions validated: {len(log_versions)} entries")

        # Check configuration files
        config_version = self._get_config_python_version()
        if config_version != current_version:
            print(f"‚ùå GATE BLOCKED: Config version {config_version} != current {current_version}")
            return False

        print(f"‚úÖ Configuration version: {config_version}")

        print("‚úÖ PYTHON VERSION CONSISTENCY: PASSED")
        return True

    def _get_current_python_version(self) -> str:
        """Get current Python version"""
        version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        return version

    def _get_venv_python_version(self) -> str:
        """Get virtual environment Python version"""
        try:
            result = subprocess.run(
                ["python", "-c", "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}')"],
                capture_output=True, text=True
            )
            return result.stdout.strip()
        except:
            return "unknown"

    def _get_logged_versions(self) -> list:
        """Get Python versions from recent logs"""
        versions = []
        log_files = Path("logs").glob("*.jsonl")

        for log_file in log_files:
            try:
                with open(log_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            log_entry = json.loads(line)
                            if 'python_version' in log_entry:
                                versions.append(log_entry['python_version'])
            except:
                continue

        return list(set(versions))  # Return unique versions

    def _get_config_python_version(self) -> str:
        """Get Python version from configuration"""
        config_file = Path(".smvm_config")
        if config_file.exists():
            with open(config_file, 'r') as f:
                for line in f:
                    if line.startswith("python_version="):
                        return line.split("=")[1].strip()
        return "unknown"

if __name__ == "__main__":
    validator = PythonVersionValidator()
    if not validator.validate_version_consistency():
        sys.exit(1)
    print("üéâ PYTHON VERSION GATE: PASSED")
```

#### Manual Verification
- [ ] Current environment uses allowed Python version
- [ ] Virtual environment matches system Python
- [ ] All logged operations use consistent Python version
- [ ] Configuration files specify correct Python version
- [ ] No version mismatches in recent runs

#### Failure Impact
**BLOCKS RELEASE**: Python version inconsistencies indicate potential compatibility issues that could cause runtime failures.

## Release Gate Execution

### Automated Gate Runner
```bash
#!/bin/bash
# release_gate_runner.sh - Automated release gate execution

echo "üö™ SMVM RELEASE GATE EXECUTION"
echo "================================"

GATE_RESULTS=()
OVERALL_STATUS="PASSED"

# Function to run a gate check
run_gate() {
    local gate_name="$1"
    local gate_script="$2"

    echo ""
    echo "Running $gate_name..."
    echo "------------------------"

    if [ -f "$gate_script" ]; then
        if bash "$gate_script"; then
            echo "‚úÖ $gate_name: PASSED"
            GATE_RESULTS+=("$gate_name:PASSED")
        else
            echo "‚ùå $gate_name: FAILED"
            GATE_RESULTS+=("$gate_name:FAILED")
            OVERALL_STATUS="BLOCKED"
        fi
    else
        echo "‚ö†Ô∏è  $gate_name: SCRIPT MISSING"
        GATE_RESULTS+=("$gate_name:MISSING")
    fi
}

# Execute all gates
run_gate "Contract Compliance" "gates/contract_gate.sh"
run_gate "Determinism Validation" "gates/determinism_gate.sh"
run_gate "Token Budget Compliance" "gates/token_gate.sh"
run_gate "Decision Quality" "gates/decision_gate.sh"
run_gate "Security & Compliance" "gates/security_gate.sh"
run_gate "Python Version Consistency" "gates/python_gate.sh"

# Summary
echo ""
echo "================================"
echo "RELEASE GATE SUMMARY"
echo "================================"

for result in "${GATE_RESULTS[@]}"; do
    gate=$(echo "$result" | cut -d: -f1)
    status=$(echo "$result" | cut -d: -f2)

    if [ "$status" = "PASSED" ]; then
        echo "‚úÖ $gate: $status"
    elif [ "$status" = "FAILED" ]; then
        echo "‚ùå $gate: $status"
    else
        echo "‚ö†Ô∏è  $gate: $status"
    fi
done

echo ""
if [ "$OVERALL_STATUS" = "PASSED" ]; then
    echo "üéâ RELEASE APPROVED"
    echo "All gates passed - system is ready for production deployment"
    exit 0
else
    echo "üö´ RELEASE BLOCKED"
    echo "One or more gates failed - address issues before proceeding"
    exit 1
fi
```

### Manual Gate Review Process
```bash
#!/bin/bash
# manual_gate_review.sh - Manual gate review checklist

echo "üìã MANUAL RELEASE GATE REVIEW"
echo "=============================="

# Load automated results
if [ -f "gate_results.json" ]; then
    AUTOMATED_PASSED=$(python -c "
import json
with open('gate_results.json') as f:
    data = json.load(f)
print('true' if data.get('overall_status') == 'PASSED' else 'false')
")
else
    AUTOMATED_PASSED="false"
fi

if [ "$AUTOMATED_PASSED" != "true" ]; then
    echo "‚ùå Cannot proceed: Automated gates have not passed"
    exit 1
fi

echo "‚úÖ Automated gates passed - proceeding with manual review"
echo ""

# Manual checklist
echo "MANUAL REVIEW CHECKLIST:"
echo "------------------------"

check_manual_item() {
    local item="$1"
    local description="$2"

    echo ""
    echo "$item: $description"
    read -p "Status (PASS/FAIL): " status

    case $status in
        [Pp][Aa][Ss][Ss])
            echo "‚úÖ $item: PASSED"
            return 0
            ;;
        [Ff][Aa][Ii][Ll])
            echo "‚ùå $item: FAILED"
            return 1
            ;;
        *)
            echo "‚ö†Ô∏è  Invalid status - assuming FAIL"
            return 1
            ;;
    esac
}

# Execute manual checks
FAILED_CHECKS=0

check_manual_item "1" "Assumptions validation - all key assumptions documented and reasonable" || ((FAILED_CHECKS++))
check_manual_item "2" "Limitations assessment - system limitations clearly documented" || ((FAILED_CHECKS++))
check_manual_item "3" "Security review - no critical security issues identified" || ((FAILED_CHECKS++))
check_manual_item "4" "Provenance verification - complete audit trail maintained" || ((FAILED_CHECKS++))
check_manual_item "5" "Python version consistency - all components use approved versions" || ((FAILED_CHECKS++))
check_manual_item "6" "Performance validation - system meets performance requirements" || ((FAILED_CHECKS++))
check_manual_item "7" "Scalability assessment - system can handle expected load" || ((FAILED_CHECKS++))
check_manual_item "8" "Monitoring readiness - adequate monitoring and alerting configured" || ((FAILED_CHECKS++))
check_manual_item "9" "Documentation completeness - all user and admin docs current" || ((FAILED_CHECKS++))
check_manual_item "10" "Rollback plan - emergency rollback procedures documented" || ((FAILED_CHECKS++))

echo ""
echo "=============================="

if [ $FAILED_CHECKS -eq 0 ]; then
    echo "üéâ MANUAL REVIEW: PASSED"
    echo "All manual checks completed successfully"
    exit 0
else
    echo "‚ùå MANUAL REVIEW: FAILED"
    echo "$FAILED_CHECKS manual checks failed - review required"
    exit 1
fi
```

## Gate Failure Response

### Immediate Actions
1. **Stop Deployment**: Immediately halt any deployment processes
2. **Log Incident**: Record gate failure with detailed diagnostics
3. **Notify Team**: Alert development and operations teams
4. **Preserve Evidence**: Save all logs, test results, and diagnostic data

### Failure Analysis Process
```python
#!/usr/bin/env python3
# gate_failure_analysis.py - Analyze gate failures

import json
from datetime import datetime
from pathlib import Path

class GateFailureAnalyzer:
    """Analyze and report on gate failures"""

    def analyze_failure(self, failed_gate: str) -> dict:
        """Analyze specific gate failure"""

        analysis = {
            "gate": failed_gate,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "root_cause": self._identify_root_cause(failed_gate),
            "impact_assessment": self._assess_impact(failed_gate),
            "remediation_steps": self._get_remediation_steps(failed_gate),
            "prevention_measures": self._get_prevention_measures(failed_gate),
            "severity": self._calculate_severity(failed_gate)
        }

        return analysis

    def _identify_root_cause(self, gate: str) -> str:
        """Identify root cause of gate failure"""

        root_causes = {
            "contract": "Schema validation or interface contract violation",
            "determinism": "Non-deterministic behavior in simulation or analysis",
            "token_budget": "Token usage exceeded established ceilings",
            "decision_quality": "Insufficient evidence or low confidence in recommendations",
            "security": "Security vulnerability or compliance violation",
            "python_version": "Python version inconsistency or unsupported version"
        }

        return root_causes.get(gate, "Unknown root cause")

    def _assess_impact(self, gate: str) -> str:
        """Assess business impact of gate failure"""

        impacts = {
            "contract": "HIGH - Interface incompatibilities could cause system failures",
            "determinism": "HIGH - Unpredictable results undermine decision confidence",
            "token_budget": "MEDIUM - Potential cost overruns and performance degradation",
            "decision_quality": "HIGH - Incorrect recommendations could cause financial loss",
            "security": "CRITICAL - Security vulnerabilities pose immediate risks",
            "python_version": "HIGH - Compatibility issues could cause runtime failures"
        }

        return impacts.get(gate, "Unknown impact")

    def _get_remediation_steps(self, gate: str) -> list:
        """Get remediation steps for gate failure"""

        remediation = {
            "contract": [
                "Review and fix schema validation errors",
                "Update interface contracts as needed",
                "Retest all contract compliance checks",
                "Update fixture data to match schemas"
            ],
            "determinism": [
                "Identify source of non-deterministic behavior",
                "Fix random seed handling in simulations",
                "Ensure deterministic algorithms are used",
                "Retest determinism across multiple runs"
            ],
            "token_budget": [
                "Audit token usage across all components",
                "Optimize prompts and reduce token consumption",
                "Adjust token ceilings if necessary",
                "Implement better token monitoring"
            ],
            "decision_quality": [
                "Review evidence collection and validation",
                "Improve confidence scoring algorithms",
                "Enhance data quality and coverage",
                "Retest decision quality metrics"
            ],
            "security": [
                "Conduct security vulnerability assessment",
                "Fix identified security issues",
                "Update security scanning tools",
                "Retest security compliance"
            ],
            "python_version": [
                "Audit Python version usage across components",
                "Ensure version consistency in all environments",
                "Update version management procedures",
                "Retest version consistency checks"
            ]
        }

        return remediation.get(gate, ["Investigate failure cause", "Implement fix", "Retest gate"])

    def _get_prevention_measures(self, gate: str) -> list:
        """Get prevention measures for future gate failures"""

        prevention = {
            "contract": [
                "Implement automated schema validation in CI/CD",
                "Regular contract testing in development workflow",
                "Schema change review process",
                "Contract versioning strategy"
            ],
            "determinism": [
                "Determinism testing in unit and integration tests",
                "Seed management best practices",
                "Algorithm determinism requirements",
                "Regular determinism audits"
            ],
            "token_budget": [
                "Token usage monitoring and alerting",
                "Budget planning for new features",
                "Token optimization reviews",
                "Cost-benefit analysis for token-intensive features"
            ],
            "decision_quality": [
                "Evidence quality gates in development",
                "Confidence threshold validation",
                "Data quality monitoring",
                "Decision validation frameworks"
            ],
            "security": [
                "Security testing in CI/CD pipeline",
                "Regular security audits and penetration testing",
                "Security training and awareness programs",
                "Security incident response procedures"
            ],
            "python_version": [
                "Version consistency checks in CI/CD",
                "Python version management policy",
                "Environment standardization",
                "Version compatibility testing"
            ]
        }

        return prevention.get(gate, ["Implement monitoring", "Add automated checks", "Regular audits"])

    def _calculate_severity(self, gate: str) -> str:
        """Calculate severity of gate failure"""

        severity_map = {
            "contract": "HIGH",
            "determinism": "HIGH",
            "token_budget": "MEDIUM",
            "decision_quality": "HIGH",
            "security": "CRITICAL",
            "python_version": "HIGH"
        }

        return severity_map.get(gate, "MEDIUM")

def main():
    """Main analysis execution"""

    if len(sys.argv) < 2:
        print("Usage: python gate_failure_analysis.py <failed_gate>")
        sys.exit(1)

    failed_gate = sys.argv[1]

    analyzer = GateFailureAnalyzer()
    analysis = analyzer.analyze_failure(failed_gate)

    print("GATE FAILURE ANALYSIS")
    print("=" * 50)
    print(f"Failed Gate: {analysis['gate']}")
    print(f"Root Cause: {analysis['root_cause']}")
    print(f"Impact: {analysis['impact_assessment']}")
    print(f"Severity: {analysis['severity']}")
    print("\nRemediation Steps:")
    for i, step in enumerate(analysis['remediation_steps'], 1):
        print(f"{i}. {step}")
    print("\nPrevention Measures:")
    for i, measure in enumerate(analysis['prevention_measures'], 1):
        print(f"{i}. {measure}")

    # Save analysis
    with open(f"gate_failure_analysis_{failed_gate}.json", "w") as f:
        json.dump(analysis, f, indent=2)

if __name__ == "__main__":
    main()
```

## Gate Success Metrics

### Quality Metrics
- **Gate Pass Rate**: Percentage of successful gate executions
- **Mean Time to Resolution**: Average time to fix gate failures
- **False Positive Rate**: Percentage of incorrect gate failures
- **Automation Coverage**: Percentage of gates with automated checks

### Process Metrics
- **Review Cycle Time**: Time from gate execution to final decision
- **Deployment Frequency**: Number of successful releases per time period
- **Rollback Rate**: Percentage of releases requiring rollback
- **Incident Rate**: Number of production incidents post-release

## Documentation and Reporting

### Gate Execution Report
```python
#!/usr/bin/env python3
# gate_execution_report.py - Generate comprehensive gate execution report

import json
from datetime import datetime
from pathlib import Path

class GateExecutionReporter:
    """Generate comprehensive gate execution reports"""

    def generate_report(self, gate_results: dict) -> str:
        """Generate HTML report of gate execution"""

        report_template = """
<!DOCTYPE html>
<html>
<head>
    <title>SMVM Release Gate Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .header { background: #f0f0f0; padding: 20px; border-radius: 5px; }
        .gate-result { margin: 20px 0; padding: 15px; border-radius: 5px; }
        .passed { background: #d4edda; border: 1px solid #c3e6cb; }
        .failed { background: #f8d7da; border: 1px solid #f5c6cb; }
        .summary { background: #e2e3e5; padding: 20px; margin: 20px 0; }
    </style>
</head>
<body>
    <div class="header">
        <h1>SMVM Release Gate Execution Report</h1>
        <p>Generated: {timestamp}</p>
        <p>Overall Status: <strong>{overall_status}</strong></p>
    </div>

    <div class="summary">
        <h2>Gate Summary</h2>
        <ul>
            {gate_summary}
        </ul>
    </div>

    <div class="results">
        <h2>Detailed Results</h2>
        {gate_details}
    </div>
</body>
</html>
"""

        # Generate gate summary
        gate_summary_items = []
        gate_details = []

        for gate_name, result in gate_results.items():
            if result['status'] == 'PASSED':
                css_class = 'passed'
                icon = '‚úÖ'
            else:
                css_class = 'failed'
                icon = '‚ùå'

            gate_summary_items.append(f"<li>{icon} {gate_name}: {result['status']}</li>")

            gate_details.append(f"""
            <div class="gate-result {css_class}">
                <h3>{gate_name}</h3>
                <p><strong>Status:</strong> {result['status']}</p>
                <p><strong>Duration:</strong> {result.get('duration', 'N/A')}</p>
                <p><strong>Details:</strong> {result.get('details', 'N/A')}</p>
            </div>
            """)

        report_html = report_template.format(
            timestamp=datetime.utcnow().isoformat(),
            overall_status=gate_results.get('overall_status', 'UNKNOWN'),
            gate_summary='\n'.join(gate_summary_items),
            gate_details='\n'.join(gate_details)
        )

        return report_html

    def save_report(self, report_html: str, filename: str = "gate_report.html"):
        """Save report to file"""

        with open(filename, 'w') as f:
            f.write(report_html)

        print(f"Gate execution report saved to: {filename}")

if __name__ == "__main__":
    # Example usage
    sample_results = {
        "overall_status": "PASSED",
        "contract_compliance": {"status": "PASSED", "duration": "2.3s", "details": "All contracts validated"},
        "determinism_validation": {"status": "PASSED", "duration": "15.7s", "details": "Determinism confirmed"},
        "token_budget": {"status": "PASSED", "duration": "1.2s", "details": "Budgets respected"},
        "decision_quality": {"status": "PASSED", "duration": "8.9s", "details": "Quality thresholds met"},
        "security_compliance": {"status": "PASSED", "duration": "5.4s", "details": "Security checks passed"},
        "python_version": {"status": "PASSED", "duration": "1.8s", "details": "Version consistency confirmed"}
    }

    reporter = GateExecutionReporter()
    report_html = reporter.generate_report(sample_results)
    reporter.save_report(report_html)
```

---

## Document Information

- **Version**: 1.0.0
- **Effective Date**: December 2, 2024
- **Last Updated**: December 2, 2024
- **Owner**: SMVM Release Engineering Team
- **Review Date**: March 2, 2025

## Appendices

### Appendix A: Gate Check Scripts
Complete collection of automated gate check scripts.

### Appendix B: Manual Review Procedures
Detailed procedures for manual gate review and approval.

### Appendix C: Failure Analysis Templates
Templates for analyzing and responding to gate failures.

### Appendix D: Success Metrics Dashboard
Guidelines for monitoring and reporting gate success metrics.

### Appendix E: Emergency Release Procedures
Procedures for emergency releases that bypass certain gates.
